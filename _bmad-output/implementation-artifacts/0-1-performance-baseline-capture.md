# Story 0.1: Performance Baseline Capture

Status: review

## Story

As a **developer**,
I want to capture performance baselines on the current codebase using the Docker dev environment,
So that I can compare post-rewrite performance against a known reference.

## Acceptance Criteria

1. **Given** the current codebase running in Docker dev environment
   **When** I run the baseline capture script
   **Then** response times are recorded for key operations (NFR1):
   - Login flow (average, p95, p99)
   - View occasion with 10+ participants
   - List ideas for user with 20+ ideas
   - Add/edit/delete idea
   - Admin: list users, list occasions

2. **Given** baseline capture completes
   **Then** results are stored in version control (`docs/performance-baseline.json`)
   **And** capture date and environment details are recorded

3. **Given** baselines are captured
   **Then** the same test scenarios are documented for post-migration comparison

## Tasks / Subtasks

- [x] Task 1: Set up k6 performance testing tool (AC: #1)
  - [x] 1.1 Add k6 to project (npm script or standalone)
  - [x] 1.2 Create `perf/` directory for performance test scripts
  - [x] 1.3 Document k6 installation in README or dev-setup

- [x] Task 2: Create k6 test script for baseline capture (AC: #1)
  - [x] 2.1 Script: Login flow scenario (`POST /api/connexion`)
  - [x] 2.2 Script: View occasion with participants (`GET /api/occasion/{id}`)
  - [x] 2.3 Script: List ideas for user (`GET /api/utilisateur/{id}/idees`)
  - [x] 2.4 Script: Add idea (`POST /api/idee`)
  - [x] 2.5 Script: Edit idea (`PUT /api/idee/{id}`)
  - [x] 2.6 Script: Delete idea (`DELETE /api/idee/{id}`)
  - [x] 2.7 Script: Admin - list users (`GET /api/utilisateurs`)
  - [x] 2.8 Script: Admin - list occasions (`GET /api/occasions`)

- [x] Task 3: Execute baseline capture on Docker dev environment (AC: #1, #2)
  - [x] 3.1 Run tests with sufficient iterations (min 100 per scenario)
  - [x] 3.2 Capture average, p95, p99 response times
  - [x] 3.3 Record environment metadata (date, Docker config, etc.)

- [x] Task 4: Store and document results (AC: #2, #3)
  - [x] 4.1 Create `docs/performance-baseline.json` with structured results - auto-generated by k6
  - [x] 4.2 Document test scenarios for reuse in Story 9.8 - done in perf/README.md
  - [x] 4.3 Commit baseline file to version control

## Dev Notes

### Brownfield Context

- **No existing performance tests** - This is the first formal performance baseline
- **Run on Docker dev environment** before any v2 code changes begin
- Results used by Story 1.0 (test infrastructure) and Story 9.8 (post-migration validation)
- This fulfills BACKLOG.md Task 25 (Performance benchmarks)
- **Open source project** - no production access assumed; dev environment is the baseline reference

### Technical Requirements

#### k6 Load Testing Tool
- **Recommended tool:** k6 (https://k6.io/)
- **Why k6:** Modern, JavaScript-based, excellent metrics, CI-friendly
- **Alternative:** Could use Apache JMeter or Artillery, but k6 preferred for simplicity

#### Test Configuration
```javascript
// k6 typical configuration for baseline
export const options = {
  iterations: 100,           // Run each scenario 100 times
  thresholds: {
    http_req_duration: ['p(95)<500', 'p(99)<1000'], // Document, don't fail
  },
};
```

### API Endpoints to Test

Based on `docs/api-reference.md`:

| Scenario | Method | Endpoint | Auth Required |
|----------|--------|----------|---------------|
| Login | POST | `/api/connexion` | No |
| View occasion | GET | `/api/occasion/{id}` | Yes |
| List user ideas | GET | `/api/utilisateur/{id}/idees` | Yes |
| Add idea | POST | `/api/idee` | Yes |
| Edit idea | PUT | `/api/idee/{id}` | Yes |
| Delete idea | DELETE | `/api/idee/{id}` | Yes |
| Admin: list users | GET | `/api/utilisateurs` | Yes (admin) |
| Admin: list occasions | GET | `/api/occasions` | Yes (admin) |

### Authentication Pattern

From API docs, use Bearer token:
```javascript
const params = {
  headers: {
    'Authorization': `Bearer ${token}`,
    'Content-Type': 'application/json',
  },
};
```

### Output Format

Create `docs/performance-baseline.json`:
```json
{
  "captured": "2026-01-XX",
  "environment": {
    "type": "docker-dev",
    "php_version": "8.4",
    "database": "mysql",
    "base_url": "http://localhost:8080"
  },
  "scenarios": {
    "login": {
      "iterations": 100,
      "avg_ms": 0,
      "p95_ms": 0,
      "p99_ms": 0
    },
    "view_occasion": { ... },
    "list_ideas": { ... },
    "add_idea": { ... },
    "edit_idea": { ... },
    "delete_idea": { ... },
    "admin_list_users": { ... },
    "admin_list_occasions": { ... }
  }
}
```

### Project Structure Notes

- Place k6 scripts in new `perf/` directory at project root
- Follow existing naming patterns (kebab-case for files)
- Results go in `docs/performance-baseline.json`

### Test Data Requirements

**Use fixtures or seeded data** for realistic baselines:
- Need occasion with 10+ participants (use fixtures or create test data)
- Need user with 20+ ideas (use fixtures or create test data)
- Need admin credentials for admin endpoints (from fixtures)
- See `docs/dev-setup.md` for loading fixtures: `./console -- fixtures`

### Environment Setup

- Run Docker dev environment: see `docs/dev-setup.md`
- Base URL: `http://localhost:8080/api`
- Ensure fixtures are loaded before running baseline capture
- k6 can run from host machine against containerized app

### References

- [Source: _bmad-output/planning-artifacts/epics.md#Story-0.1]
- [Source: _bmad-output/planning-artifacts/prd.md#Performance]
- [Source: docs/api-reference.md]
- [Source: docs/dev-setup.md] - Docker environment setup
- [Source: _bmad-output/project-context.md]

## Dev Agent Record

### Agent Model Used

Claude Opus 4.5 (claude-opus-4-5-20251101)

### Completion Notes List

**2026-01-25 - Tasks 1-2 Complete:**
- Created `./k6` Docker wrapper (follows project pattern - no host installation needed)
- Added k6 service to docker-compose.yml with tools profile
- Created `perf/baseline.js` with all 8 test scenarios
- Created `perf/README.md` with usage documentation
- Added performance testing section to `docs/testing.md`
- Fixed API endpoint paths (French naming: `/idee`, `/occasion`, `/utilisateur`)
- Fixed idea creation payload (requires `idAuteur` field)
- Verified all scenarios work with 5-iteration test run

**2026-01-25 - Tasks 3-4 Complete (Session 2):**
- Executed baseline capture with 100 iterations per scenario
- Fixed k6 script to include p99 statistics (added `summaryTrendStats` config)
- All 8 scenarios passed with 0 errors
- Results auto-generated to `docs/performance-baseline.json`
- Committed to version control (amended WIP commit)

**Baseline Results Summary:**
| Scenario | Avg | p95 | p99 |
|----------|-----|-----|-----|
| Login | 336ms | 362ms | 378ms |
| View Occasion | 87ms | 112ms | 124ms |
| List Ideas | 61ms | 76ms | 94ms |
| Add Idea | 186ms | 354ms | 1412ms |
| Edit Idea | 152ms | 258ms | 312ms |
| Delete Idea | 156ms | 254ms | 355ms |
| Admin List Users | 86ms | 105ms | 124ms |
| Admin List Occasions | 80ms | 97ms | 118ms |

### File List

Created:
- `k6` - Docker wrapper script
- `perf/baseline.js` - k6 test script (all 8 scenarios)
- `perf/README.md` - Test scenario documentation

Modified:
- `docker-compose.yml` - Added k6 service
- `docs/testing.md` - Added Performance Testing section
- `docs/dev-setup.md` - Removed duplicate perf docs (moved to testing.md)
